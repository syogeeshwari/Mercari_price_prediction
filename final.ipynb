{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"final.ipynb","provenance":[{"file_id":"1g-20SBs3Qt_Eam6FHvCfhF9HSzG8qh-G","timestamp":1590040681628},{"file_id":"1NGQjke72AS93IOpNcnE9diQEg78-sU3C","timestamp":1581417107666}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"cells":[{"cell_type":"markdown","metadata":{"id":"8GPRrm69wHn3","colab_type":"text"},"source":["#<h2>**1 Importing various libraries for the data analysis:**</h2>"]},{"cell_type":"code","metadata":{"id":"iAzdk36Nya3f","colab_type":"code","colab":{}},"source":["%matplotlib inline\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","import scipy\n","import sqlite3\n","import pandas as pd\n","import numpy as np\n","import nltk\n","import string\n","import datetime\n","import time\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import xgboost as xgb\n","import gc\n","from sklearn import preprocessing\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import shutil\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import confusion_matrix\n","from sklearn import metrics\n","from sklearn.metrics import roc_curve, auc\n","from sklearn.linear_model import SGDRegressor\n","from sklearn.model_selection import RandomizedSearchCV\n","from sklearn.metrics import mean_squared_error\n","#from py7zr import unpack_7zarchive\n","import math\n","import re\n","# Tutorial about Python regular expressions: https://pymotw.com/2/re/\n","import string\n","\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","from scipy.sparse import csr_matrix\n","from scipy.sparse import hstack\n","from sklearn.preprocessing import StandardScaler\n","from scipy.sparse import coo_matrix, hstack\n","from gensim.models import Word2Vec\n","from gensim.models import KeyedVectors\n","from prettytable import PrettyTable\n","from sklearn.linear_model import RidgeCV\n","import pickle\n","import zipfile\n","from tqdm import tqdm\n","import os\n","from sklearn.linear_model import Ridge\n","from os import path\n","from PIL import Image\n","from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n","from collections import Counter\n","import tensorflow as tf\n","from tensorflow.keras.callbacks import TensorBoard,EarlyStopping,ModelCheckpoint\n","from tensorflow.keras.layers import Input, Embedding, GRU, Dense,Flatten\n","from tensorflow.keras.models import Model,load_model\n","from numpy import zeros\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.utils import plot_model\n","from contextlib import contextmanager\n","tf.keras.backend.clear_session()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JOesRhw3ya3v","colab_type":"text"},"source":["#<h2>**2 Utility Functions:**</h2>"]},{"cell_type":"code","metadata":{"id":"sg-KYNqMya3x","colab_type":"code","colab":{}},"source":["def split_categories(category):\n","    '''\n","    function that splits the category column in the dataset and creates 3 new columns:\n","    'main_category','sub_cat_1','sub_cat_2'\n","    '''\n","    try:\n","        sub_cat_1,sub_cat_2,sub_cat_3 = category.split(\"/\")\n","        return sub_cat_1,sub_cat_2,sub_cat_3\n","    except:\n","        return (\"No label\",\"No label\",\"No label\")\n","\n","def create_split_categories(data):\n","    '''\n","    function that creates 3 new columns using split_categories function\n","    : 'main_category','sub_cat_1','sub_cat_2'\n","    '''\n","    data['main_category'],data['sub_cat_1'],data['sub_cat_2']=zip(*data['category_name'].\\\n","                                                                  apply(lambda x: split_categories(x)))\n","\n","def log_price(price):\n","    return np.log1p(price)#changes\n","\n","#https://www.kaggle.com/valkling/mercari-rnn-2ridge-models-with-notes-0-42755\n","def countwords(text):\n","    try:\n","        if text == 'No description yet':\n","            return 0\n","        else:\n","            text = text.lower()\n","            words = [w for w in text.split(\" \")]\n","            return len(words)\n","    except: \n","        return 0\n","\n","def fill_nan(dataset):\n","    '''\n","    Function to fill the NaN values in various columns\n","    '''\n","    dataset[\"item_description\"].fillna(\"No description yet\",inplace=True)\n","    dataset[\"brand_name\"].fillna(\"missing\",inplace=True)\n","    dataset[\"category_name\"].fillna(\"missing\",inplace=True)\n","\n","def rank_category(dataset,column_name):\n","    '''This function takes a column name which is categorical and returns the categories with rank'''\n","    counter = dataset[column_name].value_counts().index.values\n","    total = list(dataset[column_name])\n","    ranked_cat = {}\n","    for i in range(1,len(counter)+1):\n","        ranked_cat.update({counter[i-1] : i})\n","    return ranked_cat,len(counter)\n","\n","def encode_ranked_category(train,test,column):\n","    '''\n","    This function calls the rank_category function and returns the encoded category column    '''\n","    train[column] = train[column].astype('category')\n","    test[column] = test[column].astype('category')\n","    \n","    cat_list = list(train[column].unique())\n","    ranked_cat_tr,count = rank_category(train,column)\n","\n","    encoded_col_tr = []\n","    encoded_col_te = []\n","\n","    for category in train[column]:\n","        encoded_col_tr.append(ranked_cat_tr[category])\n","\n","    for category in test[column]:\n","        if category in cat_list:\n","            encoded_col_te.append(ranked_cat_tr[category])\n","        else:\n","            encoded_col_te.append(0)\n","    \n","    encoded_col_tr = np.asarray(encoded_col_tr)\n","    encoded_col_te = np.asarray(encoded_col_te)\n","    return encoded_col_tr,encoded_col_te,count\n","\n","def tokenize_text(train,test,column):\n","    global t\n","    t = Tokenizer()\n","    t.fit_on_texts(train[column].str.lower())\n","    vocab_size = len(t.word_index) + 1\n","    # integer encode the documents\n","    encoded_text_tr = t.texts_to_sequences(train[column].str.lower())\n","    encoded_text_te = t.texts_to_sequences(test[column].str.lower())\n","    return encoded_text_tr,encoded_text_te,vocab_size\n","\n","def data_gru(train,test):\n","    \n","    global max_length,desc_size,name_size\n","    encoded_brand_tr,encoded_brand_te,brand_len = encode_ranked_category(train,test,'brand_name')\n","    encoded_main_cat_tr,encoded_main_cat_te,main_cat_len = encode_ranked_category(train,test,'main_category')\n","    encoded_sub_cat_1_tr,encoded_sub_cat_1_te,sub_cat1_len = encode_ranked_category(train,test,'sub_cat_1')\n","    encoded_sub_cat_2_tr,encoded_sub_cat_2_te,sub_cat2_len = encode_ranked_category(train,test,'sub_cat_2')\n","    tokenized_desc_tr,tokenized_desc_te,desc_size = tokenize_text(train,test,'item_description')\n","    tokenized_name_tr,tokenized_name_te,name_size = tokenize_text(train,test,'name')\n","      \n","    max_length = 160\n","    desc_te_padded = pad_sequences(tokenized_desc_te, maxlen=max_length, padding='post')\n","    del tokenized_desc_tr,tokenized_desc_te\n","\n","    name_te_padded = pad_sequences(tokenized_name_te, maxlen=10, padding='post')\n","    del tokenized_name_tr,tokenized_name_te\n","\n","    gc.collect()\n","\n","    test_inputs = [name_te_padded,desc_te_padded,encoded_brand_te.reshape(-1,1),\\\n","                    encoded_main_cat_te.reshape(-1,1),encoded_sub_cat_1_te.reshape(-1,1),\\\n","                    encoded_sub_cat_2_te.reshape(-1,1),test['shipping'],\\\n","                    test['item_condition_id'],test['wc_desc'],\\\n","                    test['wc_name']] \n","    return test_inputs\n","\n","def rmsle_compute(y_true, y_pred):\n","    assert len(y_true) == len(y_pred)\n","    score = np.sqrt(np.mean(np.power(np.log1p(y_pred) - np.log1p(y_true), 2)))\n","    return score\n","\n","def scale_back(x):\n","    '''\n","    Function to inverse transform the scaled values\n","    '''\n","    x= np.expm1(y_scalar.inverse_transform(x.reshape(-1,1))[:,0])#changes\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mi3aiaRuya35","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":128},"executionInfo":{"status":"ok","timestamp":1594102284278,"user_tz":-330,"elapsed":28997,"user":{"displayName":"Syogee Shwari","photoUrl":"","userId":"14452843389779300235"}},"outputId":"63f3cc53-2885-4ba3-9789-6b9780f7ce0e"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6mTuj7CqMh1e","colab_type":"text"},"source":["#<h2>**3 Function 1:**</h2>"]},{"cell_type":"code","metadata":{"id":"lBQkTnRVya3-","colab_type":"code","colab":{}},"source":["def final_fun_1(x):\n","    '''This function takes in the raw input, preprocess it, trains and returns the final predictions'''\n","    ############################\n","    #step1: load the train file\n","    ############################\n","    gc.collect()\n","    train = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/self_case_study_projects/project_1/train.tsv',sep='\\t')\n","    test = x\n","    print(\"Finished loading the files....\\n\")\n","    print(\"train: {0}\\ntest: {1}\\n\".format(train.shape,test.shape))\n","    ##########################################################\n","    #step2: Data cleaning and preprocessing of train and test\n","    ##########################################################\n","    #https://www.kaggle.com/valkling/mercari-rnn-2ridge-models-with-notes-0-42755\n","    train = train.drop(train[(train.price < 3.0)].index)\n","    fill_nan(train)\n","    fill_nan(test)\n","    print(\"filled nan\\n\")\n","\n","    train['wc_desc'] = train['item_description'].apply(lambda x: countwords(x))\n","    test['wc_desc'] = test['item_description'].apply(lambda x: countwords(x))\n","    train['wc_name'] = train['name'].apply(lambda x: countwords(x))\n","    test['wc_name'] = test['name'].apply(lambda x: countwords(x))\n","    create_split_categories(train)\n","    create_split_categories(test)\n","    print(\"Completed data cleaning and preprocessing\\n\")\n","\n","    # train test split\n","    #X_train, X_test= train_test_split(train, train_size=0.99, random_state=123)\n","    print(\"shape of train: {}\".format(train.shape))\n","    print(\"shape of test: {}\".format(test.shape))\n","\n","    global y_scalar,DESC_LEN,NAME_LEN\n","    y_scalar = StandardScaler()#changes\n","    y_train = y_scalar.fit_transform(log_price(train['price']).values.reshape(-1, 1))#changes\n","\n","    DESC_LEN = train.wc_desc.max() + 1\n","    NAME_LEN = train.wc_name.max() + 1\n","    #################################\n","    #step:3 Featurizing\n","    #################################\n","    test_inputs = data_gru(train,test)\n","    #################################\n","    #step:4 compiling and predicting\n","    #################################\n","    model_1 = load_model('/content/gdrive/My Drive/Colab Notebooks/self_case_study_projects/project_1/GRU_model/final/GRU_lr-003-0.328.hdf5')\n","    te_preds_m1 = model_1.predict(test_inputs,batch_size = 2**10,verbose = 1)\n","    model_2 = load_model('/content/gdrive/My Drive/Colab Notebooks/self_case_study_projects/project_1/GRU_model/final/GRU_lr-003-0.326.hdf5')\n","    te_preds_m2 = model_1.predict(test_inputs,batch_size = 2**10,verbose = 1)\n","    del test_inputs\n","    gc.collect()\n","    #https://machinelearningmastery.com/model-averaging-ensemble-for-deep-learning-neural-networks/\n","\n","    y_hats = np.array([te_preds_m1,te_preds_m2]) #making an array out of all the predictions\n","    # mean across ensembles\n","    mean_preds = np.mean(y_hats, axis=0)\n","    return scale_back(mean_preds)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yqr7mymUObdC","colab_type":"text"},"source":["#<h3>**Price predicitons from final_func_1 with one item**</h3>\n","Giving a single test data as input for price prediction"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"XfrD5dPxQ306","colab":{"base_uri":"https://localhost:8080/","height":435},"executionInfo":{"status":"ok","timestamp":1594104690257,"user_tz":-330,"elapsed":137147,"user":{"displayName":"Syogee Shwari","photoUrl":"","userId":"14452843389779300235"}},"outputId":"2695ce3a-40d7-43a9-d851-83b44d8f636d"},"source":["#call for function_1\n","test = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/self_case_study_projects/project_1/test.tsv',sep='\\t')\n","predictions = final_fun_1(test[4:5])\n","#https://www.geeksforgeeks.org/python-convert-two-lists-into-a-dictionary/\n","result = {test[\"name\"][i]: predictions[i] for i in range(len(predictions))}\n","print(\"\\n##############################################\\n\")\n","print(\"\\nThe price predicted for the given input(s): \\n\")\n","print(\"\\nItem(s)\\tpredicted_price\\n\")\n","for k,v in result.items():\n","    print(\"{0}\\t{1}\".format(k,v))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Finished loading the files....\n","\n","train: (1482535, 8)\n","test: (1, 7)\n","\n","filled nan\n","\n","Completed data cleaning and preprocessing\n","\n","shape of train: (1481661, 13)\n","shape of test: (1, 12)\n","1/1 [==============================] - 0s 1ms/step\n","1/1 [==============================] - 0s 1ms/step\n","\n","##############################################\n","\n","\n","The price predicted for the given input(s): \n","\n","\n","Item(s)\tpredicted_price\n","\n","Breast cancer \"I fight like a girl\" ring\t8.730072021484375\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"B8bqCqgUEMdA","colab_type":"text"},"source":["#<h3>**Price predicitons from final_func_1 with more items**</h3>"]},{"cell_type":"code","metadata":{"id":"VeX2FLPZMZXP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":599},"executionInfo":{"status":"ok","timestamp":1594104185905,"user_tz":-330,"elapsed":133520,"user":{"displayName":"Syogee Shwari","photoUrl":"","userId":"14452843389779300235"}},"outputId":"7d501e86-4a9d-41e4-cda9-58a65df718d9"},"source":["predictions = final_fun_1(test[:10])\n","#https://www.geeksforgeeks.org/python-convert-two-lists-into-a-dictionary/\n","results = {test[\"name\"][i]: predictions[i] for i in range(len(predictions))}\n","print(\"\\n##############################################\\n\")\n","print(\"\\nThe price predicted for the given input(s): \\n\")\n","print(\"\\nItem(s)\\tpredicted_price\\n\")\n","for k,v in results.items():\n","    print(\"{0}\\t{1}\".format(k,v))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Finished loading the files....\n","\n","train: (1482535, 8)\n","test: (10, 7)\n","\n","filled nan\n","\n","Completed data cleaning and preprocessing\n","\n","shape of train: (1481661, 13)\n","shape of test: (10, 12)\n","1/1 [==============================] - 0s 1ms/step\n","1/1 [==============================] - 0s 1ms/step\n","\n","##############################################\n","\n","\n","The price predicted for the given input(s): \n","\n","\n","Item(s)\tpredicted_price\n","\n","Breast cancer \"I fight like a girl\" ring\t6.8716912269592285\n","25 pcs NEW 7.5\"x12\" Kraft Bubble Mailers\t11.653888702392578\n","Coach bag\t42.083763122558594\n","Floral Kimono\t12.492950439453125\n","Life after Death\t8.730072021484375\n","iPhone 6 Plus or 6s Plus Vodka pink case\t8.598246574401855\n","Vintage Cameo Pendant & Brooch Pin\t10.062675476074219\n","Rose Gold Stainless Steel Quartz Watch\t31.60715103149414\n","Daisy Marc Jacobs 3.4oz\t45.37272262573242\n","Rose Brushes and Silicone Sponge\t10.792123794555664\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6M_6qimTMsZL","colab_type":"text"},"source":["#<h2>**4 Function 2:**</h2>"]},{"cell_type":"code","metadata":{"id":"R3nNIZ_u4tRe","colab_type":"code","colab":{}},"source":["def final_fun_2(X,Y):\n","    '''This function takes in the raw input and its true target values and returns the metric on true and predicted'''\n","    score = rmsle_compute(final_fun_1(X),Y)\n","    return score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"jqjZBU6cODkT","colab":{"base_uri":"https://localhost:8080/","height":254},"executionInfo":{"status":"ok","timestamp":1594104441684,"user_tz":-330,"elapsed":137824,"user":{"displayName":"Syogee Shwari","photoUrl":"","userId":"14452843389779300235"}},"outputId":"53285a5f-6a89-4ad3-9abf-3be54c1163df"},"source":["#call for function 2\n","test1 = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/self_case_study_projects/project_1/test.tsv',sep='\\t')\n","rmsle_score = final_fun_2(test1[4:5],[13])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Finished loading the files....\n","\n","train: (1482535, 8)\n","test: (1, 7)\n","\n","filled nan\n","\n","Completed data cleaning and preprocessing\n","\n","shape of train: (1481661, 13)\n","shape of test: (1, 12)\n","1/1 [==============================] - 0s 1ms/step\n","1/1 [==============================] - 0s 1ms/step\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"w5_DxBXeOQy5","colab_type":"text"},"source":["#<h3>**RMSLE score from function 2**</h3>"]},{"cell_type":"code","metadata":{"id":"cPkkaQo3NEaB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1594105006105,"user_tz":-330,"elapsed":835,"user":{"displayName":"Syogee Shwari","photoUrl":"","userId":"14452843389779300235"}},"outputId":"80da6db3-77a9-4141-a11a-3b73333e1b07"},"source":["print(\"RMSLE score(s) for the given test data:{}\".format(rmsle_score))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["RMSLE score(s) for the given test data:0.36383598180642096\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"puZN6gLmEdU8","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}